

import csv
import xml.etree.ElementTree as ET
import operator
import time

def combine_outputs(outfilename, files_list):
    rdr = open(files_list[0], 'r')
    headers = rdr.readline()
    rdr.close()
    wtr = open(outfilename, 'w')
    wtr.write(headers)
    for filename in files_list:
        rdr = open(filename, 'r')
        data = rdr.readlines()
        rdr.close()
        wtr = open(outfilename, 'a')
        for line in data[1:]:
            wtr.write(line)

def read_csv(filename):
    rdr = csv.reader(open(filename, 'r'))
    data = []
    headers = None
    counter = 0
    for row in rdr:
        if counter % 10000 == 0:
            print counter
        if counter:
            data.append(row)                       
        else:
            headers = [x.strip() for x in row]
        counter += 1      
    return headers, data

def write_csv(filename, headers, data):
    print "Writing..."
    wtr = csv.writer(open(filename, 'wb'), delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
    wtr.writerow(headers)
    for row in data:
        wtr.writerow(row)
    del wtr
    del data

def merge_multiple_csv(filenames, combinedFileName):
    print filenames
    print combinedFileName
    _headers = []
    _data = []
    for filename in filenames:
        print filename
        print "Reading file in memory..."
        headers, data = read_csv(filename)
        _headers.append(headers)
        _data.append(data)
    merged = reduce(lambda x, y : x + y, _data)
    print _headers
    eq = True
    for member in _headers:
        if headers != member:
            eq = False
            raise ValueError("Cannot merge sheets if headers are unequal.")
    
    #print eq
    #if not reduce(operator.eq, _headers):
        #print headers
        #print reduce(operator.eq, _headers)
        #for header in _headers:
            #print len(header)
            #print header
        #print _headers
    print "Wr"
    write_csv(combinedFileName, _headers[0], merged)
    del merged
    del _headers
    del _data

#filenames = [r'C:\PyComet_v2013010_64bit\0621ms_012_010_MGFPeaklist.pep.csv', r'C:\PyComet_v2013010_64bit\0621ms_0fl_150_MGFPeaklist.pep.csv', r'C:\PyComet_v2013010_64bit\0621ms_0fl_acid_MGFPeaklist.pep.csv']
#combine_outputs(r'C:\PyComet_v2013010_64bit\Merged.pep.csv', filenames)
#merge_multiple_csv(filenames, r'C:\PyComet_v2013010_64bit\Merged.pep.csv')

def merge_csv(filename1, filename2, combinedFileName):
    print "Reading file 1 in memory..."
    headers1, data1 = read_csv(filename1)
    print "Reading file 2 in memory..."
    headers2, data2 = read_csv(filename2)
    merged = data1 + data2
    if headers1 != headers2:
        raise ValueError("Cannot merge sheets if headers are unequal.")
    write_csv(combinedFileName, headers1, merged)
    del merged
    del data1
    del data2

def calc_fdr(filename, fdr_filter = None, sort_column='expect', rev_txt='rev_gi'):
    '''
    
    Calculates fdr for COMET Search data
    
    INPUT: COMET output as csv (generated by process_file.py)
    
    If fdr filter is given a value, rows containing reverse hits or that have an fdr > the given value
    are not written to the result file.
    
    OUTPUT: csv file
    
    '''
    sort_reverse=False
    if sort_column in ['xcorr']:
        sort_reverse=True
    rdr = csv.reader(open(filename, 'r'))
    data = []
    headers = None
    counter = 0
    print "Reading file in memory..."
    for row in rdr:
        if counter:
            data.append(row)    
            data[counter-1][headers.index(sort_column)] = float(data[counter-1][headers.index(sort_column)])                   
        else:
            headers = [x.strip() for x in row] + ['fdr']
        counter += 1
        
    print "Sorting..."
    
    data.sort(key=lambda t:t[headers.index(sort_column)], reverse=sort_reverse)
    
    fwd = 0
    rev = 0
    
    print "Calc fdr..."
    for i, member in enumerate(data):
        if member[headers.index('protein')].find(rev_txt) == -1:
            fwd += 1
        else:
            rev += 1
        try:
            fdr = float(rev)/float(fwd)
        except:
            fdr = 'NA'
        data[i].append(fdr)
    
    print "Writing..."
    wtr = csv.writer(open(filename[:-4] + '.fdr.csv', 'wb'), delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
    wtr.writerow(headers)
    for row in data:
        if not fdr_filter:
            wtr.writerow(row)
        else:
            if row[headers.index('fdr')] < fdr_filter and row[headers.index('protein')].find(rev_txt) == -1:
                wtr.writerow(row)
    del wtr
    del rdr
    del data
            

def process_file(filename, first_rank_only=True):
    '''
    
    GENERATES A CSV File from a pepxml file
    
    USAGE:
    
    filename = r'C:\Data\COMET\2012-08-23-Enolase-100fmol_1_RECAL.pep.xml'
    
    process_file(filename)
    
    '''    
    tree = ET.parse(filename)
    root = tree.getroot()
    for child in root:
        for s in child.findall('.'): # {http://regis-web.systemsbiology.net/pepXML}spectrum_query
            a = s.getchildren()
    hit_list = []
    master_keys = []
    for member in a:
        if member.tag == '{http://regis-web.systemsbiology.net/pepXML}spectrum_query':
            query_data = {}
            x = member.getchildren()
            stuff = member.attrib
            query_data.update(stuff)
            for query in x:
                    z = query.getchildren() # this is search_hit
                    for y in z: # y = search_hit
                        hit_data = {}
                        for key in y.attrib.keys():
                            hit_data[key]=y.attrib[key] 
                        u = y.getchildren()
                        score_dict = {}
                        for i in u:
                            if 'name' in i.keys():
                                try:
                                    score_dict[i.attrib['name']]=i.attrib['value']
                                except:
                                    pass
                            else:
                                score_dict.update(i.attrib)
                            
                        #CONSTRUCT "HIT"
                        hit_dict = {}
                        hit_dict.update(query_data)
                        hit_dict.update(hit_data)
                        hit_dict.update(score_dict)
                        if 'xcorr' in hit_dict.keys():
                            hit_list.append(hit_dict)
                            if len(hit_dict.keys()) > len(master_keys):
                                master_keys = hit_dict.keys()
    wtr = csv.writer(open(filename[:-4] + '.csv', 'wb'), delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
    x = master_keys
    x.sort()
    #print x
    wtr.writerow(x)
    for row in hit_list:
        proceed = False
        if first_rank_only:
            if row['hit_rank'] == '1':
                proceed = True
        else:
            proceed = True
        if proceed:
            elem = []
            x = master_keys
            x.sort()
            for key in x:
                if key in row.keys():
                    elem.append(row[key])
                else:
                    elem.append('')
            wtr.writerow(elem)
    del wtr
    del hit_list
    del master_keys
    del root
    del tree

